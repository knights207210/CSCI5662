{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: Convolutional Neural Networks in Keras\n",
    "***\n",
    "\n",
    "![digits](figs/mnist.png \"mnist data\")\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "In this notebook we'll experiment with using Keras and convolutional neural networks to classify handwritten digits from the MNIST data set.  \n",
    "\n",
    "### Problem 1: Setup and Data Exploration  \n",
    "***\n",
    "\n",
    "If you do not have Keras installed on your machine, you can do this quickly and easily.  \n",
    "\n",
    "In MacOS or Linux you can install Keras using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running Windows then you can install Keras from the `conda prompt` with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conda install --channel https://conda.anaconda.org/conda-forge keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: First, we'll load the MNIST data directly from Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:30:50.951759Z",
     "start_time": "2018-04-09T19:30:48.326843Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is already partitioned into training and validation sets.  Because training CNNs is a time-intensive process, we'll reduce the size of both data sets so that they're more manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:30:52.522296Z",
     "start_time": "2018-04-09T19:30:52.518280Z"
    }
   },
   "outputs": [],
   "source": [
    "num_train, num_valid = 5000, 1000\n",
    "\n",
    "X_train = X_train[:num_train]\n",
    "y_train = y_train[:num_train]\n",
    "X_valid = X_valid[:num_valid]\n",
    "y_valid = y_valid[:num_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What are the shape and dimensions of the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: When working with CNNs we need to reshape our data to conform to general image types, which typically have multiple color bands.  We need to reshape the data arrays to have 4 dimensions, where the additional data corresponds to a single color band.  How we do this will depend on whether we're using `Theano` or `TensorFlow` as the Keras backed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:30:56.171784Z",
     "start_time": "2018-04-09T19:30:56.158394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new data dimension:  (5000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# set the size of the images \n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# Add the color-band dimension either before or after the image dimensions \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "print(\"new data dimension: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also normalize the pixel data so that intensities fall between $[0,1]$ instead of $[0,255]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:30:57.806128Z",
     "start_time": "2018-04-09T19:30:57.783723Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.0 \n",
    "X_valid = X_valid.astype('float32') / 255.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, we need to modify the data labels so that they correspond to one-hot-encoded labels.  Luckily, Keras has a function to do this for us automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:15.229278Z",
     "start_time": "2018-04-09T19:31:15.218645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example one-hot-encoded labels: \n",
      "digit label: 5, one-hot-label: [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "digit label: 0, one-hot-label: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "digit label: 4, one-hot-label: [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils \n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "y_train_onehot = np_utils.to_categorical(y_train, num_classes) \n",
    "y_valid_onehot = np_utils.to_categorical(y_valid, num_classes) \n",
    "\n",
    "print(\"example one-hot-encoded labels: \")\n",
    "for ii in range(3):\n",
    "    print(\"digit label: {:d}, one-hot-label: {}\".format(y_train[ii], y_train_onehot[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, let's look at a few of the images, just to make sure we haven't messed anything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:33.014786Z",
     "start_time": "2018-04-09T19:31:32.551077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e7c5978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6,3))\n",
    "for ii, ax in enumerate(axes):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        ax.imshow(X_train[ii,0,:,:], cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(X_train[ii,:,:,0], cmap='gray')\n",
    "    ax.set_title(\"Ground Truth : {:d}\".format(y_train[ii]))\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Our First Simple CNN\n",
    "***\n",
    "\n",
    "**Part A**: We will train a CNN with the following architecture.  Note that we're just working from good starting points here.  In practice we would want to spend time analyzing and evaluating multiple architectures, activations, and hyperparameters. \n",
    "\n",
    "Our CNN will be comprised of \n",
    "\n",
    "- 3 Convolutional Layers of increasing number of filters, each followed by a MaxPool Layer \n",
    "- A Dense hidden layer with 128 neurons and a linear activation function \n",
    "- An output layer with softmax activation \n",
    "\n",
    "First, we need to load all of the necessary modules from Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:36.363548Z",
     "start_time": "2018-04-09T19:31:36.357864Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we need to define certain hyperparameter like batch_size, number of epochs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:37.861206Z",
     "start_time": "2018-04-09T19:31:37.858457Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "num_epochs = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize a model and start adding layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:39.174050Z",
     "start_time": "2018-04-09T19:31:39.078091Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model \n",
    "simple_model = Sequential()\n",
    "\n",
    "# Convolutional layer with 32 3x3 filters followed by 2x2 MaxPool \n",
    "simple_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape,padding='same'))\n",
    "simple_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "\n",
    "# Convolutional layer with 64 3x3 filters followed by 2x2 MaxPool \n",
    "simple_model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "simple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "# Convolutional layer with 128 3x3 filters followed by 2x2 MaxPool \n",
    "simple_model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
    "simple_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "\n",
    "# Flatten and feed into dense layer \n",
    "simple_model.add(Flatten())\n",
    "simple_model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer \n",
    "simple_model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to specify the loss function and solver we'll use during training, as well as the metric that we want reported to us throughout the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:44.895645Z",
     "start_time": "2018-04-09T19:31:44.882992Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = keras.losses.categorical_crossentropy\n",
    "optimizer = keras.optimizers.Adam()\n",
    "metrics = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we tell Keras to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:46.074939Z",
     "start_time": "2018-04-09T19:31:46.054488Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model is set, Keras can print a nice summary for us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:31:47.753101Z",
     "start_time": "2018-04-09T19:31:47.745997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 356,234\n",
      "Trainable params: 356,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Now we're ready to actually train the model! You might want to grab a magazine or something because this'll take a minute!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:32:16.018342Z",
     "start_time": "2018-04-09T19:31:50.406762Z"
    }
   },
   "outputs": [],
   "source": [
    "simple_train = simple_model.fit(X_train, y_train_onehot, batch_size=batch_size, epochs=num_epochs, verbose=1,\n",
    "                                validation_data=(X_valid, y_valid_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: We can already get an idea of how we did from the training output, but let's get the final results directly from the training history and print them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:16:04.713730Z",
     "start_time": "2018-04-09T19:16:04.710323Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"training loss = {:.3f},  training accuracy = {:.3f}\".\n",
    "      format(simple_train.history['loss'][-1], simple_train.history['acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the loss and accuracy on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:18:37.160236Z",
     "start_time": "2018-04-09T19:18:37.156999Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"validation loss = {:.3f},  validation accuracy = {:.3f}\".\n",
    "      format(simple_train.history['val_loss'][-1], simple_train.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, let's make a plot of the training and validation accuracies as they evolve over the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T19:22:22.008510Z",
     "start_time": "2018-04-09T19:22:21.786060Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "ax.plot(range(1,num_epochs+1), simple_train.history['acc'], color=\"steelblue\", marker=\"o\", label=\"training\")\n",
    "ax.plot(range(1,num_epochs+1), simple_train.history['val_acc'], color=\"green\", marker=\"o\", label=\"validation\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"lower right\", fontsize=20)\n",
    "plt.xticks(range(1,num_epochs+2,2))\n",
    "ax.set_xlabel(\"epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=16)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Combating Overfitting \n",
    "*** \n",
    "\n",
    "While it doesn't appear that we really have an overfitting problem here, let's pretend that we do so we can explore methods to mitigate it.  \n",
    "\n",
    "We'll do this by adding Dropout layers after each of the Conv-MaxPool layers and after the Dense hidden layer.  You can do this by inserting layers of the form \n",
    "\n",
    "`simple_model.add(Dropout(p))`\n",
    "\n",
    "in the architecture specifications above. The parameter $p$ indicates what proportions of units to randomly ignore during training.  Typically we start with small values of $p$ and then increase them as we go farther into the network.  As a first pass, try using $p = 0.25, 0.25, 0.4, 0.3$ for the four DropOut layers. \n",
    "\n",
    "Repeat the training and evaluation process performed in **Problem 2**.  How did you do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: More Interesting Data \n",
    "*** \n",
    "\n",
    "As people became board with the vanilla MNIST data, lots of alternative data sets popped up.  A popular one is the so called `fashion_mnist` dataset.  Go back to **Problem 2** and try your network out on this data set instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
